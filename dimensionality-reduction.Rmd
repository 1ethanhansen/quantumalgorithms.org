---
title: "03-dimensionalityreduction"
author: "scinawa"
date: "12/2/2020"
output: html_document
---

# Dimensionality reduction



## Unsupervised algorithms

### Quantum PCA {#section:qpca}
Principal component analysis is a widely-used multivariate statistical method for continuous variables that finds many applications in machine learning, ranging from outlier detection to dimensionality reduction and data visualization. 
Consider a matrix $A \in \mathbb{R}^{n \times m}$ that stores information about $n$ data points using $m$ coordinates (e.g. think of $n$ images described through $m$ pixels each), its *principal components* are the set of orthogonal vectors along which the variance of the data points is maximized.
The goal of PCA is to compute the principal components, with the amount of variance they capture, and rotate the data-points to make the axis coincide with the principal components.
During the process it is possible to perform dimensionality reduction and reduce the number of variables taken into account, i.e. reducing $A \in \mathbb{R}^{n \times m}$ to $A \in \mathbb{R}^{n \times k}$ where $k\leq m$, and express the original data in terms of fewer latent variables that account for the majority of the variance of the original data. 
If it is possible to find a few latent variables that retain a large amount of variance of the original variables, it is possible to use PCA to visualize even high dimensional datasets.

##### Connection to Singular Value Decomposition
The model of PCA is closely related to the singular value decomposition of the data matrix $A$, shifted to row mean 0. 
The principal components coincide with the right singular vectors $v_i$.
The factor scores $\lambda_i = \sigma_i^2$ represent the amount of variance along each of them and the factor score ratios $\lambda^{(i)}=\frac{\lambda_i}{\sum_j^r\lambda_j}$ express the percentage of the variance retained.
For datasets with $0$ mean, the transformation consists in a rotation along the principal components: $Y = AV = U\Sigma V^TV = U\Sigma  \in \mathbb{R}^{n \times m}$.
Therefore, the data points in the new subspace can be computed using the left singular vectors and the singular values of $A$. 
When performing dimensionality reduction it suffice to use only the top $k$ singular values and vectors $Y^{(k)} = AV^{(k)} = U\Sigma V^TV^{(k)} = U^{(k)}\Sigma^{(k)}  \in \mathbb{R}^{n \times m}$.  

##### Quantum algorithms for PCA
Using the procedures from section \@ref(sec:explainedvariance) it is possible to extract the model for principal component analysis.
Theorems \@ref(thm:factor-score-estimation), \@ref(thm:check-explained-variance), \@ref(thm:explained-variance-binarysearch) allow to retrieve information on the factor scores and on the factor score ratios, while Theorem \@ref(thm:top-k-sv-extraction) allows extracting the principal components.
The run-time of the model extraction is the sum of the run-times of the theorems: $\widetilde{O}\left(\left( \frac{1}{\gamma^2} + \frac{km}{\theta\delta^2}\right)\frac{\mu(A)}{\epsilon}\right)$.
The model comes with the following guarantees:
$\|\sigma_i - \overline{\sigma}_i\| \leq \frac{\epsilon}{2}$;
$\|\lambda_i - \overline{\lambda}_i\| \leq \epsilon$;
$\|\lambda^{(i)} - \overline{\lambda}^{(i)}\| \leq \gamma$;
$\|v_i - \overline{v}_i\| \leq \delta$ for $i \in \{0,k-1\}$. 
This run-time is generally smaller than the number of elements of the input data matrix, providing polynomial speed-ups on the best classical routines for non-sparse matrices.
In writing the time complexity of the routines, we omitted the term $\frac{1}{\sqrt{p}}$ because usually the amount of variance to retain $p$ is chosen to be a number greater than 0.5  (generally in the order of 0.8/0.9). 

When performing dimensionality reduction, the goal is to obtain the matrix $Y=U\Sigma \in \mathbb{R}^{n\times k}$, where $U \in \mathbb{R}^{n \times k}$ and $\Sigma \in \mathbb{R}^{k \times k}$ are composed respectively by the top $k$ left singular vectors and singular values.
When this is the case, the user might want to extract the top $k$ $u_i$ and $\sigma_i$ rather than the principal components, to avoid matrix multiplication.
For this reason, we provide a lemma to bound the error on the retrieved mode. For sake of completeness, the error bound is also stated for $V\Sigma$.

```{lemma, accuracyUSeVS, name="Accuracy on dimensionality reduction"}
    Let $A \in \mathbb{R}^{n \times m}$ be a matrix with $\sigma_{max} \leq 1$. 
    Given some approximate procedures to retrieve estimates $\overline{\sigma}_i$ of the singular values $\sigma_i$ such that
    $\|\sigma_i - \overline{\sigma}_i\| \leq \epsilon$
    and unit estimates 
    $\overline{u}_i$
    of the left singular vectors 
    $u_i$
    such that 
    $\|\overline{u}_i - u_i\|_2 \leq \delta$,
    the error on 
    $U\Sigma$
    can be bounded as $\|U\Sigma - \overline{U}\overline{\Sigma}\|_F \leq \sqrt{k}(\epsilon+\delta)$.
    Similarly,
    $\| V\Sigma - \overline{V}\overline{\Sigma}\|_F \leq \sqrt{k}(\epsilon+\delta)$.
```
```{proof}
The first step of the proof consists in bounding the error on the columns of the matrices: $||\overline{\sigma_i}\overline{u}_i - \sigma_i u_i||$.

$$\|\overline{\sigma}_i \overline{u}_i - \sigma_i u_i\| \leq \|(\sigma_i \pm \epsilon)\overline{u}_i - \sigma_i u_i\| = \|\sigma_i (\overline{u}_i - u_i) \pm \epsilon\overline{u}_i \|$$
Because of the triangular inequality, 
$\|\sigma_i (\overline{u}_i - u_i) \pm \epsilon\overline{u}_i \| \leq \sigma_i\|\overline{u}_i - u_i\| +  \epsilon\|\overline{u}_i \|$. Also by hypothesis,  $\|(\overline{u}_i - u_i)\| \leq \delta$ and $||\overline{u}_i|| = 1$ . Thus,
$\sigma_i\| \overline{u}_i - u_i\| +  \epsilon\|\overline{u}_i \| \leq \sigma_i \delta + \epsilon$.
From the error bound on the columns and the fact that $f(x) = \sqrt{x}$ is an increasing monotone function, it is possible to prove the error bound on the matrices:
$$\|\overline{U}\overline{\Sigma} - U\Sigma\|_F = \sqrt{\sum_i^n\sum_j^k \| \overline{\sigma}_j\overline{u}_{ij} - \sigma_j u_{ij}  \|^2} = \sqrt{\sum_j^k \left( \| \overline{\sigma}_j\overline{u}_j - \sigma_j u_j \| \right)^2} $$
$$\leq \sqrt{\sum_j^k \left( \epsilon + \delta\sigma_j \right)^2} \leq 
\sqrt{k \left( \epsilon + \delta\sigma_{max} \right)^2} \leq
\sqrt{k}(\epsilon + \delta\|A\|)$$

Finally, since $\sigma_{max} \leq 1$, we get that $\|\overline{U}\overline{\Sigma} - U\Sigma\|_F \leq \sqrt{k}(\epsilon + \delta)$.
```

The fact that the matrix has been normalized to have a spectral norm smaller than one is usually not relevant for the final applications of PCA.
However, if one desires to represent transformation of the not-normalized matrix $A$, the error bounds become the ones of the lemma below.
```{lemma, unnormalizeUSeVS, name="Non-normalized accuracy for PCA dimensionality reduction"}
    The estimated representations of the lemma above, for the not-normalized matrix $A$, are 
    $\|A\|\overline{U}\overline{\Sigma}$
    and 
    $\|A\|\overline{V}\overline{\Sigma}$.
    The error bounds become $\left|\left|||A||U\Sigma - ||A||\overline{U}\overline{\Sigma}\right|\right|_F \leq \sqrt{k}||A||(\epsilon+\delta)$ and $\left|\left|||A||V\Sigma - ||A||\overline{V}\overline{\Sigma}\right|\right|_F \leq \sqrt{k}||A||(\epsilon+\delta)$
```
```{proof}
Firstly, it is easy to see that to get the desired data representations it suffices to multiply the output matrix by $\|A\|$.
Indeed, in our quantum memory, the singular values of the non-normalized matrix are scaled by a factor $\frac{1}{\|A\|}$, while the singular vectors remain the same.
To prove the bounds, we can just multiply both sides of the inequalities from Lemma \ref{Lemma:accuracyUSeVS} by $||A||$, which is a positive quantity:
$$\norm{\|A\|\overline{U}\overline{\Sigma} - ||A||U\Sigma}_F \leq \sqrt{k}\|A\|(\epsilon + \delta), \norm{\|A\|\overline{V}\overline{\Sigma} - \|A\|V\Sigma}_F \leq \sqrt{k}\|A\|(\epsilon + \delta).$$
```

## Supervised algorithms 

### Quantum Slow Feature Analysis {#section:qsfa}
Slow Feature Analysis (SFA) is a dimensionality reduction technique proposed in the context of computational neurosciences as a way to model part of the visual cortex of humans. In the last decades, it has been applied in various areas of machine learning. In this chapter we propose a quantum algorithm for slow feature analysis, and detail its application for performing dimensionality reduction on a real dataset. We also simulate the random error that the quantum algorithms might incur. We show that, despite the error caused by the algorithm, the estimate of the model that we obtain is good enough to reach high accuracy on a standard dataset widely used as benchmark in machine learning. Before providing more details on this result, we give a brief description of dimensionality reduction and introduce the model of slow feature analysis in this context. 



SFA has been shown to model a kind of neuron (called complex cell) situated in the cortical layer in the primary visual cortex (called V1) [@berkes2005rich]. SFA can be used in machine learning as a DR algorithm, and it has been successfully applied to enhance the performance of classifiers [@zhang2012slow], [@Berkes2005pattern]. SFA was originally proposed as an \emph{online, nonlinear, and unsupervised  algorithm} [@wiskott1999learning]. Its task was to learn slowly varying features from generic input signals that vary rapidly over time [@Berkes2005pattern] [@wiskott1999learning]. SFA has been motivated by the  \emph{temporal slowness principle}, that postulates that
while the primary sensory receptors (like the retinal receptors in an animal's eye) are sensitive to very small changes in the environment and thus vary on a very fast time scale, the internal representation of the environment in the brain varies on a much slower time scale. The slowness principle is a hypothesis for the functional organization of the visual cortex and possibly other sensory areas of the brain [@scholarpedia2017SFA] and it has been introduced as a way to model the transformation invariance in natural image sequences [@zhang2012slow]. SFA is an algorithm that formalizes the slowness principle as a nonlinear optimization problem. In [@blaschke2004independent, sprekeler2014extension], SFA has been used to do nonlinear blind source separation. Although SFA has been developed in the context of computational neurosciences, there have been many applications of the algorithm to solve ML related tasks. A prominent advantage of SFA compared to other algorithms is that it is almost hyperparameter-free. The only parameters to chose are in the preprocessing of the data, e.g. the initial PCA dimension and the nonlinear expansion that consists of a choice of a polynomial of (usually low) degree $p$. Another advantage is that it is guaranteed to find the optimal solution within the considered function space [@escalante2012slow]. For a detailed description of the algorithm, we suggest [@Sprekeler2008understandingframework]. With appropriate preprocessing, SFA can be used in conjunction to a supervised algorithm to acquire classification capabilities. For instance it has been used for pattern recognition to classify images of digits in the famous MNIST database [@Berkes2005pattern]. SFA can be adapted to be used to solve complex tasks in supervised learning, like face and human action recognition [@Gu2013supervised] , [@zhang2012slow [@sun2014deepsfaactionrecognition].

We can use SFA for classification in the following way. One can think of the training set a set of vectors $x_i \in \mathbb{R}^d , i \in n$. Each $x_i$ belongs to one of $K$ different classes. A class $T_k$ has $|T_k|$ vectors in it. The goal is to learn $K-1$ functions $g_j( x_i), j \in [K-1]$ such that the output $y_i = [g_1( x_i), \cdots , g_{K-1}(  x_i )]$ is very similar for the training samples of the same class and largely different for samples of different classes. Once these functions are learned, they are used to map the training set in a low dimensional vector space. When a new data point arrive, it is mapped to the same vector space, where classification can be done with higher accuracy. SFA projects the data points onto the subspace spanned by the eigenvectors associated to the $k$ smallest eigenvalues of the derivative covariance matrix of the data, which we define in the next section.

#### The SFA model

Now we introduce the optimization problem in its most general form as it is commonly stated for classification [@Berkes2005pattern].  Let $a=\sum_{k=1}^{K} {\binom{ |T_k| }{2}}.$
\noindent
For all $j \in [K-1]$, minimize:
$$\Delta(y_j) =  \frac{1}{a} \sum_{k=1}^K \sum_{\substack{s,t \in T_k \\ s<t}} \left( g_j( x_s) - g_j( x_t) \right)^2$$
with  the following constraints:

- $\frac{1}{n} \sum_{k=1}^{K}\sum_{i\in T_k} g_j( x_i) = 0 \quad  \forall j \in [K-1]$
- $\frac{1}{n} \sum_{k=1}^{K}\sum_{i \in T_k} g_j( x_i)^2 = 1 \quad  \forall j \in [K-1]$
- $\frac{1}{n} \sum_{k=1}^{K}\sum_{i \in T_k} g_j( x_i)g_v( x_i) = 0 \quad \forall v < j$


The minimization of the delta values $\Delta(y_j)$ encodes the requirement on the output signal to vary  "as slow as possible", and thus the delta values are our measure of slowness. They are the average of the square of the first order derivative (over time) of the $j$-th component of the output signal $y_t$. The first requirement states that the average over time of each component of the signal should be zero, and it is stated just for convenience, such that the other two requirements take a simple form. The second requirement asks for the variance over time of each component of the signal to be $1$. It is equivalent to saying that each signal should carry some information and avoid the trivial solution $g_j(\cdot) = 0$. The third requirement is to say that we want the signals to be decorrelated with each other. This also introduces an order, such that the first signal is the slowest, the second signal is the second slowest and so on. The first and the second constraint also avoid the trivial solution $y_i = 0$. Intuitively, the decorrelation constraint forces different functions $g_j$ to encode different ``aspects'' of the input, maximizing the information carried by the output signal. 


In order for the minimization problem to be computationally feasible at scale, the $g_j$'s are restricted to be  linear functions $w_j$ such that the output signal becomes $y_i = [w_1^T x_i, \cdots  w_{K-1}^T x_i ]^T$ or else $Y=XW$, where $X \in \mathbb{R}^{n \times d}$ is the matrix with rows the input samples and $W \in \mathbb{R}^{d \times (K-1)}$ the matrix that maps the input matrix $X$ into a lower dimensional output $Y \in \mathbb{R}^{n \times (K-1)}$. In case it is needed to capture nonlinear relations in the dataset, one performs a standard nonlinear polynomial expansion on the input data as preprocessing. Usually, a polynomial expansion of degree $2$ or $3$ is chosen. For example we can take:

$$x=[x_1, x_2, x_3] \to \left[x_1^2, x_1x_2, x_1x_3, x_2^2, x_2x_3, x_3^2,x_1, x_2, x_3 \right].$$

The choice of the nonlinear expansion is important for using SFA in machine learning contexts. If it is a low dimensional expansion, it might not solve the task with high accuracy, while if the dimension is too high, it might overfit the training data, and therefore not generalize properly to test data. This technique also goes under the name of polynomial kernel.

We also need to satisfy the constraint on the average of the signal being zero and have unit variance. This is not a strong requirement, since it can be enforced beforehand, by preprocessing the dataset. This requires only linear time with respect to the dimensions of the data, and in practice consist in removing the mean and scale the components by their variance. Namely, we assume that the $j$-th component of the $i$-th vector in the dataset satisfies the condition:

<!-- % $$ x_j(i) := \frac{\tilde{x}_j(i) - E_i[\tilde{x}_j(i)] }{ \sqrt{ E_i [ ( \tilde{x}_j (i) - E_i[\tilde{x}_j(i)] )^2 ] } }, $$ -->


$$(x_i)_j := \frac{ (\tilde{x}_i)_j - E[(\tilde{x}_i)_j] }{ \sqrt{ E [ ( ( \tilde{x}_i)_j - E[( \tilde{x}_i)_j] )^2 ] } },$$

where with $\tilde{x}(i)$ we define a raw signal with arbitrary mean and variance, $E[\tilde x_j(i)]$ the expected value of a single component of the vectors. This allows us to rewrite the minimization problem including the constraints of zero mean and unit variance. We can restate the definition of the delta function as the the GEP in Optimization Form \@ref(eq:opt1):

\begin{equation}\label{delta}
\Delta(y_j) = \frac{ w^T_j A  w_j}{  w_j^T B  w_j} ,
\end{equation}

where the matrix $B$ is called the sample covariance matrix and defined as:

\begin{equation} \label{defB}
B:= \frac{1}{n}\sum_{i \in [n]} x_i x_i^T = X^T X
\end{equation}

and the matrix $A$ is called the sample derivative covariance matrix and defined as:

\begin{equation} \label{defA}
A := \frac{1}{a} \sum_{k=1}^K         \sum_{\substack{i, i' \in T_k \\ i < i'}} (  x_i -  x_{i'} )(  x_i -  x_{i'} )^T = \frac{1}{a} \sum_{k=1}^K    \dot{X_k}^T \dot{X_k} :=  \dot{X}^T \dot{X}.
\end{equation}

Note also, that we can approximate the matrix $A$ by subsampling from all possible pairs $(x_i, x_{i'})$ from each class and this is indeed what happens in practice.

#####  Slowly varying signals
We formalize the concept of slowly varying signals. While this won't have any utility in the classical algorithm, it will allow us to bound nicely the runtime of the quantum algorithm, in the case when the data has ``structure'' that can be extracted by the SFA algorithm. In general, a slow signal is a signal that change slowly over time. This concept is formalized in the context of SFA by requiring that the whitened signal can be reconstructed without too much error from the projection on a few slow feature vectors. Formally, for a given $K$, and a set of slow feature vectors $w_1 \dots w_{K-1}$, we define a slowly varying signal as follows.


```{definition, slowlyvarying, name="Slowly varying signal"}
Let $X \in \mathbb{R}^{n \times d}$ and $Y \in [K]^{n}$ be a dataset and its labels. Let the rows $x_i$ of $X$ have whitened representation $z_i$. For the $K-1$ slow feature vectors $w_j, j \in [K]$, let $y_i$ be the slow feature representation of $x_i$. We say that $X$ is $\gamma_K$-slow if:
$$\frac{\sum_{i=0}^n \norm{z_i}}{\sum_{i=0}^n \norm{y_i}} \leq \gamma_{K}$$
```

If we use SFA for doing dimensionality reduction in the context of supervised learning, a dataset is slowly varying if all the elements in the same class are similar to each other. In this way, by projecting the original images in the subspace spanned by a small number of slow feature vectors, we can reconstruct most of the information of the original dataset. We stress that this assumption is not needed for the quantum algorithm to work, but instead it will only be useful to give guarantees on the runtime of the algorithm.


#####  The SFA algorithm
The SFA algorithm basically provides a solution to the generalized eigenvalue problem $AW=\Lambda BW$ and outputs the eigenvectors corresponding to the smallest eigenvalues. As we said we assume that the data has been normalized and polynomially expanded. 

The first step of the algorithm is to whiten the data. This will reduce the problem into a normal eigenvalue problem; the second step is to perform PCA in order to find the eigenvalues and eigenvectors. We refer to  [@escalante2012slow] for a more complete description.

######  Step 1: Whitening the data
Recall that $X \in \mathbb{R}^{n \times d}, A,B \in \mathbb{R}^{d \times d}$. We now show how to whiten the data by right multiplying with the matrix $B^{-1/2} = [ (X^TX)]^{-1/2}$. Then, the input matrix becomes $Z=XB^{-1/2}$ and the covariance matrix of the whitened data $Z^TZ$ is thus the identity.

```{lemma}
Let $Z:=XB^{-1/2} $ be the matrix of whitened data. Then $Z^TZ = I$.% and $B^{-1/2}$ is symmetric.
```
```{proof}
Let $X=U\Sigma V^T$. We defined $B=V\Sigma^{2}V^T$. As $Z=U\Sigma V^T(V\Sigma^{-1}V^T)=UIV$
It follows that $Z^TZ=I$.
```

As in the classical algorithm, we will whiten the data by left-applying the whitening matrix $B^{-1/2}$. We will use matrix multiplication algorithms to create a state $\ket{Z}$ proportional to the whitened data.

######  Step 2: Projection in slow feature space

The second step of SFA consists of outputting the $K-1$ ``slowest'' eigenvectors of the derivative covariance matrix of the whitened data $A :=\dot{Z}^T\dot{Z}$, where $\dot{Z}$ is defined similar to $\dot{X}$ by using the whitened samples instead of the original ones. Note that taking the derivatives of the whitened data is equal to whitening the derivatives.

```{lemma, commutingderivatives}
Let $A = \dot{Z}^T\dot{Z}$. Then $A = (B^{-1/2})^T \dot{X}^T \dot{X}B^{-1/2}$.
```

```{proof}
\begin{eqnarray*}
A & = & \dot{Z}^T\dot{Z} = \frac{1}{a} \sum_{k=1}^K         \sum_{\substack{i, i' \in T_k \\ i < i'}} (z_i -  z_{i'} )(  z_i -  z_{i'} )^T \\
& = & (B^{-1/2})^T   \frac{1}{a} \sum_{k=1}^K         \sum_{\substack{i, i' \in T_k \\ i < i'}} ( x_i - x_{_i'} )(x_i - x_{i'} )^T  B^{-1/2} \\
& = & (B^{-1/2})^T  \dot{X}^T \dot{X} B^{-1/2}
\end{eqnarray*}
```

This observation allow us to whiten the data with a quantum procedure. Recall that the matrix $A$ is usually approximated with a small fraction of all the possible derivatives, roughly linear (and not quadratic) on the number of data points. In our case we take the number of rows of the derivative matrix to be just double the number of data points, and in the experiment we show that this does not compromise the accuracy.


<!-- \begin{algorithm}[ht] -->
<!-- 	\floatname{algorithm}{SFA - Algorithm}  -->
<!-- 	\caption{(Classical) Slow Feature Analysis}\label{alg_classical_sfa} -->
<!-- 	\begin{algorithmic}[1] -->
<!-- 		\Require -->
<!-- 		\Statex Input $X \in \mathbb{R}^{n \times d}$ (normalized and polynomially expanded), and $K < d \in \mathbb{N}$ -->
<!-- 		\Ensure -->
<!-- 		\Statex $Y=ZW$, where $Z=XB^{-1/2}$ is the whitened input signal, and $W \in \mathbb{R}^{d \times (K-1)}$ are the $K-1$ eigenvectors of the matrix $A=\dot{Z}^T \dot{Z}$ corresponding to the smallest eigenvalues -->
<!-- 		\Statex  -->
<!-- %		\State \ale{Normalize the vector's component to $0$ mean and variance $1$ : -->
<!-- %		 $$ \bm x_i(t) := \frac{\tilde{\bm x}_i(t) - \langle \tilde{\bm x_i} \rangle }{ \sqrt{ \langle ( \tilde{\bm x (t)}_i - \langle \tilde{\bm x}_i \rangle )^2 \rangle } } $$ -->
<!-- %		} -->
<!-- %		\State \ale{ Apply a nonlinear expansion $h$, bringing each vector $\bm x(t)$ to a higher dimensional space $\bm e(t) = h(\bm x(t))$. \ale{Call $E$ the matrix of polynomially expanded vectors.} -->
<!-- %		} -->
<!-- 		\State Whiten the signal: $Z := X B^{-1/2}$, and create $\dot{Z}$ from $Z$. -->
<!-- 		\State Perform PCA on the derivative covariance matrix $A=\dot{Z}^T\dot{Z}$ of the whitened data. -->
<!-- 		\State Return $Y = ZW$, the projection of whitened data onto $W$, the $K-1$ slowest eigenvectors of $A$ -->

<!-- 	\end{algorithmic} -->
<!-- \end{algorithm}  -->



#### Quantum Slow Feature Analysis {#sec:qsfa}
We are finally ready to put forward a quantum procedure for SFA. Specifically, we want to map the input matrix $X$ to the output matrix $Y=XW$, and then we will see how to estimate classically the slow feature vectors $W \in\mathbb{R}^{(K-1)\times d}$. For this, we assume to have quantum access to the matrices $X$ and $\dot{X}$, as in definition \ref{def:QRAM}. We start by describing a circuit that approximately performs the unitary $U_{QSFA} : \ket{X}\to \ket{Y}$ where $\ket{X}$ is the quantum state we obtain by having quantum access to $X$, the dataset, and $\ket{Y} :=  \frac{1}{\norm{Y}_F} \sum_{i=0}^n\norm{y_i}\ket{i}\ket{y_i}$.
 As the classical algorithm, QSFA is divided in two parts. In the first step we whiten the data, i.e. we map the state $\ket{X}$ to the state $\ket{Z}=\ket{XB^{-1/2}}$, and in the second step we approximately project $\ket{Z}$ onto the subspace spanned by the smallest eigenvectors of the whitened derivative covariance matrix $A=\dot{Z}^T\dot{Z}$.



#### Step 1: Whitening the data

Recall that $X = \sum_i \sigma_i u_iv_i^T \in \mathbb{R}^{n\times d},$ and $A,B \in \mathbb{R}^{d \times d}$. We now show how to whiten the data having quantum access to the matrix $X$.  As $B^{-1/2}$ is a symmetric matrix with eigenvectors the column singular vectors of $X$ and eigenvalues equal to $1/|\sigma_i|$. Using quantum linear algebra procedure, i.e. theorem \@ref(thm:qla), we can multiply with $B^{-1/2}$ our state $\ket{X}$. Thus, we have the following corollary.


```{corollary, name="Whitening algorithm"}
Assume to have quantum access to $X = \sum_i \sigma_i u_iv_i^T \in \mathbb{R}^{n\times d}$, as in theorem \@ref(qram).
Let $Z = XB^{-1/2}$ the matrix of whitened data. There exists a quantum algorithm that produces as output a state $\ket{\bar{Z}}$ such that $| \ket{\bar{Z}} - \ket{Z}| \leq \varepsilon$ in expected time $\tilde{O}(\kappa(X)\mu(X) \log{1/\varepsilon}))$.
```
#### Step 2: Projection in slow feature space


The previous Corollary gives a way to build quantum access to the rows of the whitened matrix  $Z$, up to some error $\epsilon$. Now we want to project this state onto the subspace  spanned by the eigenvectors associated to the $K-1$ ``slowest'' eigenvectors of the whitened derivative covariance matrix $A :=\dot{Z}^T\dot{Z}$, where
$\dot{Z}$ is the whitened derivative matrix $\dot{Z} = \dot{X}B^{-1/2}$. Let $\theta$ a threshold value and $\delta$ a precision parameter, that governs the error we tolerate in the projection threshold.
Recall that $A_{\leq \theta, \delta}$ we denote a projection of the matrix $A$ onto the vector subspace spanned by the union of the singular vectors associated to singular values that are smaller than $\theta$ and some subset of singular vectors whose corresponding singular values are in the interval $[\theta, (1+\delta) \theta]$.


To perform the projection, we will need a threshold for the eigenvalues that will give us the subspace of the $K-1$ slowest eigenvectors. A priori, we don't know the appropriate threshold value, and thus it must be found experimentally through binary search since it depends on the distribution of singular values of the matrix representing the dataset.  We can now describe and analyse the entire QSFA algorithm.


As in the previous section, we note that the eigenvalues of $A_{\dot{Z}}$ are the squares of the singular values of $\dot{Z}$, and the two matrices share the same column space: $\dot{Z} = U\Sigma V^T$, and $A_{\dot{Z}} = V\Sigma^2 V^T$. Claim \@ref(commutingderivatives) tells us that whitening the derivatives of the signal is equal to taking the derivatives of the whitened data.  theorem \@ref(thm:qlap) provides exactly the procedure for accessing the rows of $\dot{Z}$, since we know how to multiply with $\dot{X}$ and with $B^{-1/2}$.

<!--  \begin{algorithm}[H] -->
<!--  \floatname{algorithm}{QSFA} -->
<!--  \label{alg_QSFA} -->
<!--  \caption{Quantum Slow Feature Analysis} -->
<!--  \begin{algorithmic}[1] -->
<!--  \Require -->
<!--        \Statex Quantum access to matrices $X \in \mathbb{R}^{n \times d}$ and $\dot{X} \in \mathbb{R}^{n \times d}$, parameters $\epsilon, \theta,\delta,\eta >0$.  -->
<!--      \Ensure -->
<!--        \Statex A state $\ket{\overline{Y}}$ such that $ | \ket{Y} - \ket{\overline{Y}} | \leq \epsilon$, with $Y = A^+_{\leq \theta, \delta}A_{\leq \theta, \delta} Z$ -->
<!--        \Statex -->

<!--  \State Create the state $\ket{X} :=  \frac{1}{\norm{X}_F} \sum_{i=1}^{n} \norm{x_i} \ket{i}\ket{x_i} $ -->

<!--  \State (Whitening algorithm) Map $\ket{X}$ to $\ket{\overline{Z}}$ with $| \ket{\overline{Z}}  - \ket{Z} | \leq \epsilon $ and $Z=XB^{-1/2}.$ -->

<!--  \State (Projection in slow feature space) Use theorem \ref{th:qla} project $\ket{\overline{Z}}$ onto the slow eigenspace of $A$ using threshold $\theta$ and precision $\delta$ (i.e. $A^+_{\leq \theta, \delta}A_{\leq \theta, \delta}\overline{Z}$)  -->

<!--  \State Perform amplitude amplification and estimation on the register $\ket{0}$ with the unitary $U$ implementing steps 1 to 3, to obtain  -->
<!--  $\ket{\overline{Y}}$ with $| \ket{\overline{Y}} - \ket{Y}  | \leq \epsilon $ and an estimator $\overline{\norm{Y}}$  with multiplicative error $\eta$. -->
<!--  \end{algorithmic} -->
<!--  \end{algorithm}  -->

We conclude this section by stating the main dimensionality reduction theorem of this paper.



```{theorem, qsfa, name="QSFA algorithm"}
Assume to have quantum access to $X = \sum_i \sigma_i u_iv_i^T \in \mathbb{R}^{n\times d}$ and its derivative matrix
 $\dot{X} \in \mathbb{R}^{n \log n \times d}$. Let $\epsilon, \theta, \delta, \eta >0$.
 There exists a quantum algorithm that produces as output a state $\ket{\overline{Y}}$ with
 $| \ket{\overline{Y}} - \ket{A^+_{\leq \theta, \delta}A_{\leq \theta, \delta} Z} | \leq \epsilon$
 in time
$$\tilde{O}\left(  \frac{ ( \kappa(X) + \kappa(\dot{X})) ( \mu({X})+ \mu(\dot{X}) ) }{\delta\theta}  \gamma_{K-1} \right)$$

and an estimator $\overline{\norm{Y}}$ with $| \overline{\norm{Y}} - \norm{Y} | \leq \eta \norm{Y}$
 with an additional $1/\eta$ factor.
```

```{proof}
QSFA consists of two steps. The first step is the whitening, which can be performed in time $\tilde{O}(\kappa(X)\mu(X)\log(1/\epsilon))$ and provide the state $\ket{\overline{Z}}$ using Corollary \ref{coro:whitening}. It is simple to verify that creating a state $\ket{Z}$ of whitened data such that $Z^TZ = I$ can be done using quantum access just to the matrix $X$, as $Z=XB^{-1/2}$. The second step is the projection of whitened data in the slow feature space, which is spanned by the eigenvectors of $A=\dot{Z}^T\dot{Z}$. This matrix shares the same right eigenvectors of $\dot{X}B^{-1/2}$, which is simple to check that we can efficiently access using the QRAM constructions of $X$ and $\dot{X}$. Using the algorithm for quantum linear algebra, i.e. theorem \ref{th:qlap}, we know that the projection (without the amplitude amplification) takes time equal to the ratio $\mu(X) +\mu(\dot{X})$ over the threshold parameter, in other words it takes time $\tilde{O}( \frac{(\mu({X})+ \mu(\dot{X}) }{\delta \theta})$. Finally, the amplitude amplification and estimation depends on the size of the projection of $\ket{\overline{Z}}$ onto the slow eigenspace of $A$, more precisely it corresponds to the factor $O(\frac{\norm{\overline{Z}}}{ \norm{A^+_{\leq \theta, \kappa}A_{\leq \theta, \kappa} \overline{Z} }})$, which is roughly the same if we look at $Z$ instead of $\overline{Z}$. Note also that $Z$ is the whitened data, which means that each whitened vector should look roughly the same on each direction. This implies that the ratio should be proportional to the ratio of the dimension of the whitened data over the dimension of the output signal. 	The final runtime of the algorithm is:

$$\tilde{O}\left(  \left( \kappa(X)\mu(X)\log (1/\varepsilon) + \frac{ ( \kappa(X) + \kappa(\dot{X})) ( \mu({X})+ \mu(\dot{X}) ) }{\delta\theta} \right)
 \frac{\norm{Z}}{ \norm{A^+_{\leq \theta, \delta}A_{\leq \theta, \delta} {Z} }} \right)$$
Note that the last ratio in this runtime was defined as $\gamma_{K-1}$ in definition \@ref(def:slowlyvarying). From this, the runtime in the statement of the theorem follows.
```
